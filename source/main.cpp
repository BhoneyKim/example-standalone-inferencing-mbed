#include "mbed.h"
#include "mbed_mem_trace.h"
#include "ei_run_classifier.h"
#include "numpy.hpp"

static const int MAX_NUMBER_STRING_SIZE = 32;
static char s[MAX_NUMBER_STRING_SIZE];

// this is the features array from the DSP block
static const float features[] = {
    0.7961, 0.4667, 0.3020, 0.3098, 0.5490, 0.4706, 0.4431, 0.4471, 0.4353, 0.4157, 0.3922, 0.3608, 0.3647, 0.3843, 0.4275, 0.5255, 0.5961, 0.6471, 0.6706, 0.6902, 0.7098, 0.7255, 0.7373, 0.7490, 0.7569, 0.7569, 0.7608, 0.7569, 0.7490, 0.7490, 0.7529, 0.7608, 0.6980, 0.4196, 0.2941, 0.3412, 0.5608, 0.4471, 0.4471, 0.4392, 0.4235, 0.4078, 0.3843, 0.3490, 0.3490, 0.3725, 0.4235, 0.5216, 0.5961, 0.6510, 0.6745, 0.6902, 0.7059, 0.7294, 0.7451, 0.7490, 0.7529, 0.7569, 0.7529, 0.7451, 0.7451, 0.7451, 0.7451, 0.7529, 0.5922, 0.3922, 0.2902, 0.3804, 0.5647, 0.4392, 0.4510, 0.4392, 0.4235, 0.4118, 0.3804, 0.3529, 0.3686, 0.3882, 0.4353, 0.5216, 0.5961, 0.6510, 0.6745, 0.6941, 0.7098, 0.7294, 0.7412, 0.7490, 0.7569, 0.7529, 0.7412, 0.7412, 0.7412, 0.7373, 0.7451, 0.7490, 0.5529, 0.3843, 0.2824, 0.4196, 0.5608, 0.4431, 0.4510, 0.4392, 0.4275, 0.4078, 0.3922, 0.3098, 0.2510, 0.2627, 0.3765, 0.5451, 0.6000, 0.6471, 0.6784, 0.6980, 0.7137, 0.7255, 0.7373, 0.7451, 0.7490, 0.7451, 0.7412, 0.7412, 0.7412, 0.7373, 0.7451, 0.7490, 0.4549, 0.3765, 0.2824, 0.4745, 0.5529, 0.4471, 0.4510, 0.4431, 0.4275, 0.4157, 0.2627, 0.0902, 0.0431, 0.0353, 0.0627, 0.2824, 0.5961, 0.6510, 0.6667, 0.6941, 0.7098, 0.7216, 0.7373, 0.7412, 0.7490, 0.7490, 0.7451, 0.7412, 0.7373, 0.7373, 0.7412, 0.7451, 0.3882, 0.3765, 0.2902, 0.5059, 0.5294, 0.4510, 0.4510, 0.4431, 0.4431, 0.3020, 0.0667, 0.0431, 0.0353, 0.0314, 0.0275, 0.0235, 0.2745, 0.6431, 0.6588, 0.6784, 0.6980, 0.7176, 0.7333, 0.7412, 0.7490, 0.7490, 0.7412, 0.7333, 0.7294, 0.7255, 0.7333, 0.7412, 0.4510, 0.3608, 0.3020, 0.5294, 0.5137, 0.4549, 0.4510, 0.4510, 0.4039, 0.1216, 0.0314, 0.0314, 0.0235, 0.0196, 0.0196, 0.0431, 0.0431, 0.4196, 0.6824, 0.6706, 0.6980, 0.7176, 0.7294, 0.7412, 0.7451, 0.7490, 0.7333, 0.7255, 0.7216, 0.7216, 0.7294, 0.7412, 0.4627, 0.3529, 0.3137, 0.5647, 0.5059, 0.4549, 0.4510, 0.4588, 0.2784, 0.0471, 0.0275, 0.0235, 0.0196, 0.0196, 0.0196, 0.0353, 0.0431, 0.1804, 0.6235, 0.6706, 0.6902, 0.7216, 0.7294, 0.7412, 0.7451, 0.7412, 0.7333, 0.7255, 0.7176, 0.7176, 0.7176, 0.7294, 0.4275, 0.3529, 0.3412, 0.5922, 0.4980, 0.4588, 0.4588, 0.4353, 0.1765, 0.0549, 0.0196, 0.0196, 0.0196, 0.0196, 0.0157, 0.0235, 0.0549, 0.0980, 0.5020, 0.6863, 0.6745, 0.7098, 0.7255, 0.7412, 0.7490, 0.7412, 0.7294, 0.7216, 0.7176, 0.7098, 0.7137, 0.7216, 0.4196, 0.3490, 0.3608, 0.6039, 0.4902, 0.4510, 0.4667, 0.3765, 0.1686, 0.1216, 0.0078, 0.0235, 0.0196, 0.0196, 0.0196, 0.0235, 0.0471, 0.0706, 0.4157, 0.6824, 0.6549, 0.6902, 0.7176, 0.7412, 0.7490, 0.7451, 0.7255, 0.7176, 0.7098, 0.7020, 0.7059, 0.7176, 0.4235, 0.3490, 0.3882, 0.6196, 0.4863, 0.4588, 0.4784, 0.3098, 0.2235, 0.1961, 0.0039, 0.0235, 0.0196, 0.0196, 0.0196, 0.0275, 0.0392, 0.0706, 0.3922, 0.6510, 0.6471, 0.6863, 0.7098, 0.7294, 0.7451, 0.7412, 0.7255, 0.7059, 0.6902, 0.6863, 0.6941, 0.7059, 0.4235, 0.3490, 0.4157, 0.6275, 0.4784, 0.4627, 0.4863, 0.2863, 0.2431, 0.2039, 0.0039, 0.0235, 0.0196, 0.0196, 0.0196, 0.0275, 0.0353, 0.0863, 0.4196, 0.6353, 0.6431, 0.6863, 0.7137, 0.7216, 0.7373, 0.7373, 0.7137, 0.6941, 0.6824, 0.6784, 0.6784, 0.6941, 0.4314, 0.3490, 0.4392, 0.6314, 0.4706, 0.4667, 0.4824, 0.2902, 0.2588, 0.1922, 0.0039, 0.0235, 0.0196, 0.0196, 0.0235, 0.0314, 0.0353, 0.1294, 0.4941, 0.6235, 0.6431, 0.6902, 0.7137, 0.7216, 0.7373, 0.7333, 0.7098, 0.6902, 0.6784, 0.6706, 0.6745, 0.6863, 0.4275, 0.3529, 0.4588, 0.6314, 0.4706, 0.4706, 0.4824, 0.3294, 0.2431, 0.1922, 0.0510, 0.0353, 0.0157, 0.0235, 0.0275, 0.0314, 0.0431, 0.2314, 0.5765, 0.6157, 0.6510, 0.6902, 0.7137, 0.7176, 0.7333, 0.7333, 0.7059, 0.6941, 0.6745, 0.6627, 0.6627, 0.6745, 0.4275, 0.3569, 0.4824, 0.6392, 0.4706, 0.4784, 0.4745, 0.4118, 0.1333, 0.0745, 0.0667, 0.0235, 0.0196, 0.0235, 0.0275, 0.0392, 0.0784, 0.3961, 0.6078, 0.6118, 0.6588, 0.6941, 0.7176, 0.7137, 0.7294, 0.7294, 0.7098, 0.6941, 0.6745, 0.6549, 0.6510, 0.6627, 0.4314, 0.3569, 0.5020, 0.6392, 0.4745, 0.4824, 0.4667, 0.4706, 0.1608, 0.0824, 0.1059, 0.0353, 0.0196, 0.0196, 0.0392, 0.0588, 0.2314, 0.5529, 0.6039, 0.6275, 0.6667, 0.7020, 0.7216, 0.7176, 0.7255, 0.7294, 0.7059, 0.6941, 0.6784, 0.6549, 0.6392, 0.6510, 0.4392, 0.3569, 0.5176, 0.6471, 0.4784, 0.4863, 0.4706, 0.4745, 0.3490, 0.1451, 0.0824, 0.0824, 0.0235, 0.0353, 0.0667, 0.1765, 0.4784, 0.5961, 0.6000, 0.6392, 0.6745, 0.7098, 0.7216, 0.7176, 0.7333, 0.7294, 0.7098, 0.6941, 0.6784, 0.6471, 0.6235, 0.6392, 0.4392, 0.3608, 0.5255, 0.6549, 0.4784, 0.4941, 0.4745, 0.4706, 0.4275, 0.0902, 0.0431, 0.0706, 0.0314, 0.1020, 0.2118, 0.4392, 0.5725, 0.5922, 0.6196, 0.6510, 0.6824, 0.7176, 0.7255, 0.7176, 0.7333, 0.7294, 0.7059, 0.6902, 0.6627, 0.6275, 0.6078, 0.6157, 0.4549, 0.3725, 0.5255, 0.6588, 0.4824, 0.4941, 0.4824, 0.4745, 0.4431, 0.0980, 0.0118, 0.1333, 0.1647, 0.3294, 0.4471, 0.5333, 0.5725, 0.6157, 0.6431, 0.6627, 0.6980, 0.7294, 0.7373, 0.7333, 0.7373, 0.7255, 0.7059, 0.6824, 0.6471, 0.6157, 0.5882, 0.5922, 0.4627, 0.3804, 0.5294, 0.6588, 0.4863, 0.5020, 0.4863, 0.4784, 0.4627, 0.2667, 0.2863, 0.3020, 0.2941, 0.4745, 0.4824, 0.5451, 0.6000, 0.6353, 0.6667, 0.6824, 0.7098, 0.7412, 0.7529, 0.7490, 0.7451, 0.7412, 0.7098, 0.6824, 0.6510, 0.6196, 0.5922, 0.5882, 0.4667, 0.3922, 0.5294, 0.6627, 0.4902, 0.5059, 0.4902, 0.4863, 0.4353, 0.3176, 0.4588, 0.3176, 0.3137, 0.4863, 0.4941, 0.5725, 0.6275, 0.6667, 0.6902, 0.7098, 0.7294, 0.7569, 0.7725, 0.7686, 0.7686, 0.7569, 0.7294, 0.7020, 0.6745, 0.6392, 0.6196, 0.6118, 0.4667, 0.3961, 0.5294, 0.6706, 0.4980, 0.5059, 0.4980, 0.4980, 0.4431, 0.3490, 0.4549, 0.2667, 0.2000, 0.4745, 0.5176, 0.5804, 0.6471, 0.6902, 0.7137, 0.7333, 0.7569, 0.7725, 0.7843, 0.7882, 0.7882, 0.7765, 0.7529, 0.7294, 0.6980, 0.6667, 0.6471, 0.6392, 0.4588, 0.3882, 0.5137, 0.6745, 0.5020, 0.5098, 0.5020, 0.5020, 0.4549, 0.3765, 0.4667, 0.1804, 0.0627, 0.4745, 0.5333, 0.5922, 0.6588, 0.7020, 0.7216, 0.7490, 0.7647, 0.7725, 0.8000, 0.7961, 0.7961, 0.7922, 0.7647, 0.7490, 0.7216, 0.6902, 0.6745, 0.6667, 0.4314, 0.3647, 0.4627, 0.6471, 0.5098, 0.5098, 0.5020, 0.5059, 0.4471, 0.3922, 0.4392, 0.2392, 0.1176, 0.4588, 0.5490, 0.6039, 0.6667, 0.7137, 0.7412, 0.7569, 0.7725, 0.7843, 0.8039, 0.8039, 0.8039, 0.8039, 0.7882, 0.7686, 0.7373, 0.7137, 0.7020, 0.6902, 0.4157, 0.3451, 0.4118, 0.6275, 0.5098, 0.5020, 0.4980, 0.5098, 0.4353, 0.3882, 0.4157, 0.2667, 0.1686, 0.4000, 0.5647, 0.6118, 0.6745, 0.7176, 0.7490, 0.7608, 0.7725, 0.7922, 0.8078, 0.8118, 0.8118, 0.8039, 0.7961, 0.7804, 0.7490, 0.7216, 0.7098, 0.7098, 0.4039, 0.3373, 0.3765, 0.6157, 0.5137, 0.4980, 0.4941, 0.5059, 0.4353, 0.3882, 0.3765, 0.3176, 0.2196, 0.3373, 0.5765, 0.6118, 0.6745, 0.7176, 0.7451, 0.7608, 0.7686, 0.7882, 0.8078, 0.8157, 0.8196, 0.8196, 0.7961, 0.7843, 0.7569, 0.7255, 0.7137, 0.7216, 0.3922, 0.3373, 0.3569, 0.6078, 0.5137, 0.4980, 0.4980, 0.5059, 0.4431, 0.3882, 0.3255, 0.3725, 0.2667, 0.2902, 0.5725, 0.6157, 0.6824, 0.7255, 0.7569, 0.7686, 0.7686, 0.7961, 0.8118, 0.8157, 0.8275, 0.8275, 0.7961, 0.7725, 0.7569, 0.7255, 0.7137, 0.7216, 0.4078, 0.3373, 0.3412, 0.6118, 0.5255, 0.4941, 0.5059, 0.5137, 0.4510, 0.3804, 0.3059, 0.4157, 0.3098, 0.2745, 0.5529, 0.6275, 0.6824, 0.7294, 0.7608, 0.7765, 0.7765, 0.8078, 0.8157, 0.8235, 0.8275, 0.8275, 0.8000, 0.7765, 0.7608, 0.7294, 0.7176, 0.7294, 0.4314, 0.3451, 0.3294, 0.6039, 0.5412, 0.4980, 0.5059, 0.5137, 0.4706, 0.3608, 0.3098, 0.4745, 0.3412, 0.2706, 0.5216, 0.6471, 0.6980, 0.7412, 0.7686, 0.7804, 0.7804, 0.8078, 0.8196, 0.8314, 0.8392, 0.8392, 0.8078, 0.7843, 0.7647, 0.7373, 0.7255, 0.7333, 0.3922, 0.3608, 0.3137, 0.5765, 0.5569, 0.4902, 0.5059, 0.5137, 0.4941, 0.3529, 0.2745, 0.3569, 0.2000, 0.2118, 0.4902, 0.6588, 0.7059, 0.7569, 0.7843, 0.7882, 0.7882, 0.8196, 0.8275, 0.8392, 0.8510, 0.8471, 0.8118, 0.7922, 0.7647, 0.7412, 0.7294, 0.7373, 0.3569, 0.3725, 0.3059, 0.5412, 0.5765, 0.4863, 0.5098, 0.5098, 0.5216, 0.3294, 0.2235, 0.2941, 0.2235, 0.2000, 0.4235, 0.6706, 0.7098, 0.7686, 0.7961, 0.7961, 0.7961, 0.8275, 0.8275, 0.8431, 0.8549, 0.8392, 0.8118, 0.7922, 0.7647, 0.7412, 0.7255, 0.7294, 0.3176, 0.3882, 0.2941, 0.5020, 0.5922, 0.4824, 0.5059, 0.5020, 0.5373, 0.2941, 0.3647, 0.5412, 0.3804, 0.3882, 0.3608, 0.6706, 0.7137, 0.7647, 0.8000, 0.7961, 0.7961, 0.8235, 0.8314, 0.8392, 0.8549, 0.8392, 0.8118, 0.7922, 0.7608, 0.7373, 0.7255, 0.7216
};

static bool debug2 = true;

void print_memory_info() {
    // Grab the heap statistics
    mbed_stats_heap_t heap_stats;
    mbed_stats_heap_get(&heap_stats);
    printf("Heap size: %lu / %lu bytes (max: %lu)\r\n", heap_stats.current_size, heap_stats.reserved_size, heap_stats.max_size);
}

void ei_printf_float(float f)
{
    float n = f;

    static double PRECISION = 0.00001;

    if (n == 0.0) {
        strcpy(s, "0");
    }
    else {
        int digit, m;
        char *c = s;
        int neg = (n < 0);
        if (neg) {
            n = -n;
        }
        // calculate magnitude
        m = log10(n);
        if (neg) {
            *(c++) = '-';
        }
        if (m < 1.0) {
            m = 0;
        }
        // convert the number
        while (n > PRECISION || m >= 0) {
            double weight = pow(10.0, m);
            if (weight > 0 && !isinf(weight)) {
                digit = floor(n / weight);
                n -= (digit * weight);
                *(c++) = '0' + digit;
            }
            if (m == 0 && n > 0) {
                *(c++) = '.';
            }
            m--;
        }
        *(c) = '\0';
    }

    printf("%s", s);
}

int main() {
    printf("Edge Impulse standalone inferencing (Mbed)\n");
    print_memory_info();

    while (1) {
        printf("Hello world\n");

        TfLiteStatus init_status = trained_model_init(ei_aligned_malloc);
        if (init_status != kTfLiteOk) {
            printf("Failed to allocate TFLite arena (error code %d)\n", init_status);
        }
        else {
            printf("OK allocating TFLite arena\n");
        }

        ei_impulse_result_t result = { 0 };

        TfLiteTensor* input = trained_model_input(0);
        TfLiteTensor* output = trained_model_output(0);

        printf("Filling %d bytes (byte size of input tensor %u)\n", sizeof(features) / sizeof(features[0]),
            input->bytes);

        for (size_t ix = 0; ix < sizeof(features) / sizeof(features[0]); ix++) {
            input->data.int8[ix] = static_cast<int8_t>(round(features[ix] / EI_CLASSIFIER_TFLITE_INPUT_SCALE) + EI_CLASSIFIER_TFLITE_INPUT_ZEROPOINT);
        }

        printf("invoke\n");

        uint64_t ctx_start_ms = ei_read_timer_ms();

        trained_model_invoke();

        uint64_t ctx_end_ms = ei_read_timer_ms();

        printf("done invoke\n");

        result.timing.classification = ctx_end_ms - ctx_start_ms;

        // Read the predicted y value from the model's output tensor
        if (debug2) {
            ei_printf("Predictions (time: %d ms.):\n", result.timing.classification);
        }
        bool int8_output = output->type == TfLiteType::kTfLiteInt8;
        for (uint32_t ix = 0; ix < EI_CLASSIFIER_LABEL_COUNT; ix++) {
            float value;
            // Dequantize the output if it is int8
            if (int8_output) {
                value = static_cast<float>(output->data.int8[ix] - output->params.zero_point) * output->params.scale;
            } else {
                value = output->data.f[ix];
            }
            if (debug2) {
                ei_printf("%s:\t", ei_classifier_inferencing_categories[ix]);
                ei_printf_float(value);
                ei_printf("\n");
            }
            result.classification[ix].label = ei_classifier_inferencing_categories[ix];
            result.classification[ix].value = value;
        }

        printf("Predictions (DSP: %d ms., Classification: %d ms., Anomaly: %d ms.): \n",
            result.timing.dsp, result.timing.classification, result.timing.anomaly);

        // print the predictions
        printf("[");
        for (size_t ix = 0; ix < EI_CLASSIFIER_LABEL_COUNT; ix++) {
            ei_printf_float(result.classification[ix].value);
#if EI_CLASSIFIER_HAS_ANOMALY == 1
            printf(", ");
#else
            if (ix != EI_CLASSIFIER_LABEL_COUNT - 1) {
                printf(", ");
            }
#endif
        }
#if EI_CLASSIFIER_HAS_ANOMALY == 1
        ei_printf_float(result.anomaly);
#endif
        printf("]\n");

        print_memory_info();

        trained_model_reset(ei_aligned_free);

        printf("After trained_model_reset\n");

        print_memory_info();

        ThisThread::sleep_for(5000);
    }
}
